#!/usr/bin/env python
"""
Pipeline Trigger Service

This script monitors directories for new images and processes them directly.
It uses the existing Monitor class from the gppy codebase.
"""

import time
import logging
import glob
from pathlib import Path
from gppy.services.monitor import Monitor
from gppy.wrapper import DataReduction
from gppy.services.queue import QueueManager
from gppy.const import RAWDATA_DIR


# Set up logging with rotation
from logging.handlers import RotatingFileHandler

# Create rotating file handler (max 10MB per file, keep 5 backup files = 50MB total)
file_handler = RotatingFileHandler(
    "/var/log/pipeline-trigger.log",
    maxBytes=10 * 1024 * 1024,  # 10MB
    backupCount=5,  # Keep 5 backup files
)

file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))

# Console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))

# Configure root logger
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, console_handler],
)
logger = logging.getLogger(__name__)


def check_too(image_path):
    file_path = Path(image_path)
    rawdata_path = Path(RAWDATA_DIR)
    relative_path = file_path.relative_to(rawdata_path)
    path_parts = relative_path.parts
    if path_parts and path_parts[0].startswith("7DT"):
        if path_parts[1].endswith("_ToO"):
            return True
        else:
            return False


def get_too_images(image_path):
    """
    Get images from the corresponding ToO directory for a non-ToO image.
    Returns list of image paths if ToO directory exists, None otherwise.
    """
    file_path = Path(image_path)
    rawdata_path = Path(RAWDATA_DIR)
    relative_path = file_path.relative_to(rawdata_path)
    path_parts = list(relative_path.parts)
    if path_parts and path_parts[0].startswith("7DT"):
        if not path_parts[1].endswith("_ToO"):
            # Construct path to corresponding ToO directory
            too_dir_name = path_parts[1] + "_ToO"
            too_path = rawdata_path / path_parts[0] / too_dir_name
            if too_path.exists():
                image_list = list(glob.glob(str(too_path / "*.fits")))
                return image_list if image_list else None
    return None


def monitor_session_completion(session_name, job_id, start_time, num_images, is_too, completion_file):
    """
    Monitor a completion marker file and log when processing completes.
    Runs in background thread to avoid blocking.
    """
    import threading
    from datetime import datetime

    def check_completion():
        # Wait for the completion file to be created (check every 10 minutes)
        completion_file_path = Path(completion_file)
        max_wait = 86400  # Max 24 hours
        check_interval = 600  # 10 minutes in seconds
        waited = 0

        while waited < max_wait:
            time.sleep(check_interval)
            waited += check_interval

            if completion_file_path.exists():
                # Read completion status
                try:
                    with open(completion_file_path, "r") as f:
                        content = f.read().strip()
                        # File contains: EXIT_CODE|DURATION|TIMESTAMP
                        if "|" in content:
                            exit_code, duration, timestamp = content.split("|", 2)
                            exit_code = int(exit_code)
                            duration = float(duration)

                            end_time = datetime.now()
                            status = "successfully" if exit_code == 0 else "with errors"
                            logger.info(
                                f"Processing completed {status} for session {session_name} (job_id: {job_id}) - "
                                f"Duration: {duration:.1f}s ({int(duration//60)}m {int(duration%60)}s), "
                                f"Images: {num_images}, ToO: {is_too}, Exit code: {exit_code}"
                            )
                        else:
                            # Fallback if format is unexpected
                            logger.info(
                                f"Processing completed for session {session_name} (job_id: {job_id}) - "
                                f"Images: {num_images}, ToO: {is_too}"
                            )
                except Exception as e:
                    logger.warning(f"Could not read completion file {completion_file}: {e}")

                # Clean up completion file
                try:
                    completion_file_path.unlink()
                except Exception:
                    pass

                break

        if waited >= max_wait:
            logger.warning(
                f"Monitoring timeout for session {session_name} (job_id: {job_id}) - "
                f"no completion detected after {max_wait}s"
            )

    # Start monitoring in background thread
    thread = threading.Thread(target=check_completion, daemon=True)
    thread.start()


def run_image_processing(image_paths, is_too=False):

    import subprocess
    import uuid
    from datetime import datetime
    import os

    # Create a unique identifier for this processing job
    job_id = str(uuid.uuid4())[:8]
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    date = datetime.now().strftime("%Y%m%d")
    session_name = f"monitor_{date}_{job_id}"
    start_time = datetime.now()

    logger.info(
        f"Processing {len(image_paths)} new images (job_id: {job_id}, session: {session_name}) from {timestamp}"
    )

    # Create completion marker file path
    completion_file = f"/tmp/pipeline_completion_{job_id}.txt"

    # Ensure /tmp/pipeline directory exists
    os.makedirs("/tmp/pipeline", exist_ok=True)

    # Save image list to temporary file
    imlist_file = f"/tmp/pipeline/imlist_{job_id}.txt"
    with open(imlist_file, "w") as f:
        for image_path in image_paths:
            f.write(f"{image_path}\n")

    logger.info(f"Saved image list to {imlist_file}")

    # Create a script to run the processing
    script_content = f"""#!/bin/bash
cd /tmp/pipeline
source ~/.bashrc
export PYTHONPATH=/home/pipeline/pipeline
export CONDA_PREFIX=/home/pipeline/.conda/envs/pipeline
export CONDA_DEFAULT_ENV=pipeline

echo "Starting processing of {len(image_paths)} images..."
echo "Session: {session_name}"
echo "Job ID: {job_id}"
echo ""

# Run the processing and capture exit code
START_TIME=$(date +%s)
/home/pipeline/.conda/envs/pipeline/bin/python -c "
from gppy.wrapper import DataReduction
from gppy.services.queue import QueueManager
import sys
import time

# Read image paths from file
with open('{imlist_file}', 'r') as f:
    image_paths = [line.strip() for line in f if line.strip()]

dr = DataReduction.from_list(image_paths, is_too={is_too})
dr.create_config(is_too={is_too})
queue = QueueManager(monitor=True)
dr.process_all(queue=queue, is_too={is_too})

"
EXIT_CODE=$?
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
END_TIMESTAMP=$(date +"%Y-%m-%d %H:%M:%S")

# Write completion status to file for monitoring
echo "$EXIT_CODE|$DURATION|$END_TIMESTAMP" > {completion_file}

# Clean up temporary image list file
rm -f {imlist_file}

echo ""
echo "=========================================="
if [ $EXIT_CODE -eq 0 ]; then
    echo "Processing completed successfully (exit code: $EXIT_CODE)"
else
    echo "Processing failed or was interrupted (exit code: $EXIT_CODE)"
fi
echo "Completed at: $END_TIMESTAMP"
echo "=========================================="
echo ""
echo "Session will remain open. Press Ctrl+D or type 'exit' to close."
echo ""

# Keep session alive - wait for user input or keep shell open
exec bash
"""

    # Create temporary script file
    import tempfile

    with tempfile.NamedTemporaryFile(mode="w", suffix=".sh", delete=False) as f:
        f.write(script_content)
        script_path = f.name

    import os

    os.chmod(script_path, 0o755)

    # Create new tmux session and run the processing script
    subprocess.Popen(["tmux", "new-session", "-d", "-s", session_name, script_path])

    logger.info(f"Started processing in tmux session: {session_name}")
    logger.info(f"Attach with: tmux attach -t {session_name}")

    # Start monitoring session completion in background
    monitor_session_completion(session_name, job_id, start_time, len(image_paths), is_too, completion_file)


def process_new_images(image_paths):
    """
    Process new images using the existing pipeline infrastructure.
    Each image set runs in its own tmux session for isolation and monitoring.

    Logic:
    - Images in _ToO directories: process with is_too=True
    - Images in non-_ToO directories: add corresponding ToO images and process with is_too=False
    """
    try:
        # Separate ToO and non-ToO images
        too_image_paths = [image for image in image_paths if check_too(image)]
        non_too_image_paths = [image for image in image_paths if not check_too(image)]

        # Process ToO images with is_too=True
        if len(too_image_paths) > 0:
            run_image_processing(too_image_paths, is_too=True)

        # Process non-ToO images: add corresponding ToO images and process with is_too=False
        if len(non_too_image_paths) > 0:
            # Collect all ToO images for non-ToO directories
            all_too_images = set()
            for image in non_too_image_paths:
                too_images = get_too_images(image)
                if too_images:
                    all_too_images.update(too_images)

            # Combine non-ToO images with their corresponding ToO images
            combined_images = non_too_image_paths + list(all_too_images)
            if len(combined_images) > 0:
                run_image_processing(combined_images, is_too=False)

    except Exception as e:
        logger.error(f"Error processing images: {e}", exc_info=True)


def start_trigger_monitoring():
    """Start the trigger monitoring service using the existing Monitor class."""
    # RAWDATA_DIR = "/lyman/data2/obsdata/"
    # Create the monitor using the existing codebase
    monitor = Monitor(base_path=Path(RAWDATA_DIR))
    monitor.add_callback(process_new_images)
    observer = monitor.start()

    logger.info("Trigger service started - monitoring directories for new images")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("Trigger service stopping...")
        observer.stop()

    observer.join()
    logger.info("Trigger service stopped")


if __name__ == "__main__":
    start_trigger_monitoring()
