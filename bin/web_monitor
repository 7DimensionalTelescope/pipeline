#!/usr/bin/env python
import os

# os.environ["SCRIPT_DIR"] = "/data/pipeline_reform/gppy"
# os.environ["REF_DIR"] = "/data/pipeline_reform/gppy/gppy/ref"
# os.environ["RAWDATA_DIR"] = "/data/pipeline_reform/obsdata_test"
# os.environ["PROCESSED_DIR"] = "/data/pipeline_reform/processed_test_light"
# os.environ["MASTER_FRAME_DIR"] = "/data/pipeline_reform/master_frame_test"
# os.environ["FACTORY_DIR"] = "/data/pipeline_reform/factory_test"
# os.environ["SLACK_TOKEN"] = "xoxb-4343183012295-8382638967284-a2oYskQhBRzUdgCdeNPRUOMB"

import time
import numpy as np
import logging
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from gppy.run import run_scidata_reduction, run_masterframe_generator
from pathlib import Path
from gppy.services.queue import QueueManager, Priority

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("/var/log/pipeline_monitor.log"),
        logging.StreamHandler(),
    ],
)


class PipelineHandler(FileSystemEventHandler):
    """Handler for pipeline directory events"""

    def __init__(self, queue):
        self.processed_files = set()  # Track processed files
        self.queue = queue

    def process_file(self, file_path):
        """Process a single request file"""
        filename = os.path.basename(file_path)

        if not filename.startswith("request_"):
            return

        if file_path in self.processed_files:
            return

        try:
            logging.info(f"Loading parameters from {file_path}")
            obs_params = np.load(file_path, allow_pickle=True).item()
            logging.info(f"Parameters {obs_params}")
            masterframe = obs_params.pop("masterframe", False)
            if masterframe:
                self.queue.add_task(
                    run_masterframe_generator,
                    args=(obs_params,),
                    kwargs={"queue": False, "overwrite": True},
                    task_name=file_path,
                    gpu=True,
                )
            else:
                self.queue.add_task(
                    run_scidata_reduction,
                    args=(obs_params,),
                    kwargs={
                        "queue": False,
                        "processes": ["preprocess", "astrometry", "photometry"],
                        "overwrite": True,
                    },
                    task_name=file_path,
                )

            self.processed_files.add(file_path)
            logging.info(f"Successfully processed {file_path}")

        except Exception as e:
            logging.error(f"Error processing {file_path}: {str(e)}")
            raise  # Re-raise to ensure systemd captures the exit code

    def on_created(self, event):
        """Handle file creation events"""
        logging.info(f"Event: {event}")  # Changed from logger to logging
        if event.is_directory:
            return
        file_path = event.src_path
        logging.info(f"File created: {file_path}")  # Changed from logger to logging
        if os.path.basename(file_path).startswith("request_"):
            time.sleep(0.1)  # Ensure file is fully written
            self.process_file(file_path)

    def on_modified(self, event):
        """Handle file modification events"""
        if event.is_directory:
            return
        file_path = event.src_path
        if os.path.basename(file_path).startswith("request_"):
            self.process_file(file_path)


def monitor_pipeline(directory="/tmp/pipeline"):
    """Monitor the pipeline directory for request files using watchdog"""
    Path(directory).mkdir(parents=True, exist_ok=True)

    queue = QueueManager(max_workers=20)

    event_handler = PipelineHandler(queue)
    observer = Observer()
    observer.schedule(event_handler, directory, recursive=False)
    observer.start()

    logging.info(f"Starting pipeline monitor in {directory}")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        logging.info("Pipeline monitor stopped by user")
    except Exception as e:
        logging.error(f"Fatal error: {str(e)}")
    finally:
        observer.join()


if __name__ == "__main__":
    try:
        monitor_pipeline()
    except ImportError as e:
        logging.error(
            f"Missing dependency: {str(e)}. Please install required packages."
        )
        raise
    except Exception as e:
        logging.error(f"Startup error: {str(e)}")
        raise
